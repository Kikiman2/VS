{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eae81a4b",
   "metadata": {},
   "source": [
    "# Image → FEN Sequence Model\n",
    "\n",
    "This notebook trains a CNN encoder + sequence decoder model that predicts a FEN-like string from a schematic image of a chess board. It uses filenames (without extension) as labels, where each filename encodes the board position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e26e6185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.13.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "print('TensorFlow version:', tf.__version__)\n",
    "\n",
    "# Avoid OOM errors by setting GPU Memory Consumption Growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus: \n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7053ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version: 2.13.1\n",
      "Num GPUs: 0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TF version:\", tf.__version__)\n",
    "print(\"Num GPUs:\", len(tf.config.list_physical_devices(\"GPU\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3c1fdc",
   "metadata": {},
   "source": [
    "## 1. Paths and label extraction\n",
    "\n",
    "We assume the following directory structure:\n",
    "\n",
    "- `data/train`: training images\n",
    "- `data/val`: validation images\n",
    "- `data/test`: test images\n",
    "\n",
    "Each image filename (without extension) is a FEN-like string, e.g.:\n",
    "`1b1B1b2-2pK2q1-4p1rB-7k-8-8-3B4-3rb3.jpeg` → label string `1b1B1b2-2pK2q1-4p1rB-7k-8-8-3B4-3rb3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb452f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = Path('data')\n",
    "train_dir = data_root / 'train'\n",
    "val_dir   = data_root / 'val'\n",
    "test_dir  = data_root / 'test'\n",
    "\n",
    "def get_image_label_pairs(folder: Path, exts=(\".jpeg\", \".jpg\", \".png\")):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    if not folder.exists():\n",
    "        raise FileNotFoundError(f\"Folder not found: {folder}\")\n",
    "    for p in folder.iterdir():\n",
    "        if p.suffix.lower() in exts and p.is_file():\n",
    "            image_paths.append(str(p))\n",
    "            labels.append(p.stem)  # filename without extension\n",
    "    return image_paths, labels\n",
    "\n",
    "train_paths, train_labels = get_image_label_pairs(train_dir)\n",
    "val_paths,   val_labels   = get_image_label_pairs(val_dir)\n",
    "test_paths,  test_labels  = get_image_label_pairs(test_dir)\n",
    "\n",
    "print(len(train_paths), 'train images')\n",
    "print(len(test_paths),  'test images')\n",
    "print('Example label:', train_labels[0] if train_labels else 'N/A')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ec7287",
   "metadata": {},
   "source": [
    "## 2. Character-level vocabulary\n",
    "\n",
    "We build a character-level vocabulary from all FEN-like labels. We also add special tokens: `<PAD>`, `<SOS>`, `<EOS>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e5eddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = train_labels + val_labels + test_labels\n",
    "if not all_labels:\n",
    "    raise ValueError('No labels found. Check that your data folders contain images.')\n",
    "\n",
    "chars = sorted(list({c for lab in all_labels for c in lab}))\n",
    "print('Chars in dataset:', chars)\n",
    "\n",
    "PAD_TOKEN = '<PAD>'\n",
    "SOS_TOKEN = '<SOS>'\n",
    "EOS_TOKEN = '<EOS>'\n",
    "\n",
    "vocab = [PAD_TOKEN, SOS_TOKEN, EOS_TOKEN] + chars\n",
    "char2idx = {c: i for i, c in enumerate(vocab)}\n",
    "idx2char = {i: c for c, i in char2idx.items()}\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print('Vocab size:', vocab_size)\n",
    "\n",
    "max_len_raw = max(len(l) for l in all_labels)\n",
    "max_len = max_len_raw + 2  # +2 for SOS and EOS\n",
    "print('Max sequence length:', max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ca13c4",
   "metadata": {},
   "source": [
    "## 3. Encode labels to integer sequences\n",
    "\n",
    "For each label string we build:\n",
    "\n",
    "- **Decoder input**: `[SOS, c1, c2, ..., cN]`\n",
    "- **Target output**: `[c1, c2, ..., cN, EOS]`\n",
    "\n",
    "Both are padded to `max_len`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62e6802",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_label(label: str):\n",
    "    chars_list = list(label)\n",
    "    # decoder input: SOS + label chars\n",
    "    in_seq = [char2idx[SOS_TOKEN]] + [char2idx[c] for c in chars_list]\n",
    "    # target: label chars + EOS\n",
    "    out_seq = [char2idx[c] for c in chars_list] + [char2idx[EOS_TOKEN]]\n",
    "\n",
    "    # pad or truncate\n",
    "    in_seq  = in_seq[:max_len]  + [char2idx[PAD_TOKEN]] * max(0, max_len - len(in_seq))\n",
    "    out_seq = out_seq[:max_len] + [char2idx[PAD_TOKEN]] * max(0, max_len - len(out_seq))\n",
    "\n",
    "    return np.array(in_seq, dtype=np.int32), np.array(out_seq, dtype=np.int32)\n",
    "\n",
    "# quick sanity check\n",
    "test_in, test_out = encode_label(all_labels[0])\n",
    "print('Example encoded input length:', len(test_in))\n",
    "print('Example encoded target length:', len(test_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6eb6088",
   "metadata": {},
   "source": [
    "## 4. `tf.data` pipeline\n",
    "\n",
    "We create a dataset that yields:\n",
    "\n",
    "- Inputs: `{ 'image': image_tensor, 'decoder_input': token_ids }`\n",
    "- Target: `decoder_output` token IDs.\n",
    "\n",
    "Images are resized to 256×256 and normalized to `[0, 1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1673b402",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 256\n",
    "\n",
    "\n",
    "def load_image(path):\n",
    "    img_bytes = tf.io.read_file(path)\n",
    "    # Use decode_jpeg instead of decode_image so TF knows the rank\n",
    "    img = tf.image.decode_jpeg(img_bytes, channels=3)\n",
    "    # Explicitly set shape: [H, W, 3]\n",
    "    img.set_shape([None, None, 3])\n",
    "\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "    return img\n",
    "\n",
    "\n",
    "def encode_label_tf(label):\n",
    "    in_seq, out_seq = tf.py_function(\n",
    "        func=lambda s: encode_label(s.numpy().decode('utf-8')),\n",
    "        inp=[label],\n",
    "        Tout=[tf.int32, tf.int32]\n",
    "    )\n",
    "    in_seq.set_shape((max_len,))\n",
    "    out_seq.set_shape((max_len,))\n",
    "    return in_seq, out_seq\n",
    "\n",
    "def make_dataset(paths, labels, batch_size=32, shuffle=False):\n",
    "    paths_ds = tf.data.Dataset.from_tensor_slices(paths)\n",
    "    labels_ds = tf.data.Dataset.from_tensor_slices(labels)\n",
    "    ds = tf.data.Dataset.zip((paths_ds, labels_ds))\n",
    "\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(paths))\n",
    "\n",
    "    def _process(path, label):\n",
    "        img = load_image(path)\n",
    "        dec_in, dec_out = encode_label_tf(label)\n",
    "        return {'image': img, 'decoder_input': dec_in}, dec_out\n",
    "\n",
    "    ds = ds.map(_process, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "batch_size = 32\n",
    "train_ds = make_dataset(train_paths, train_labels, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for batch in train_ds.take(1):\n",
    "    x, y = batch\n",
    "    print('Image batch shape:', x['image'].shape)\n",
    "    print('Decoder input shape:', x['decoder_input'].shape)\n",
    "    print('Decoder target shape:', y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f5a1d0",
   "metadata": {},
   "source": [
    "## 5. CNN encoder\n",
    "\n",
    "We use EfficientNetB0 (ImageNet-pretrained) as the image encoder, followed by a dense layer to produce a 256-dimensional embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd8a158",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_encoder(img_size=IMG_SIZE, embed_dim=256):\n",
    "    base = tf.keras.applications.EfficientNetB0(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_shape=(img_size, img_size, 3)\n",
    "    )\n",
    "    base.trainable = False  # start frozen; fine-tune later if needed\n",
    "\n",
    "    inputs = layers.Input(shape=(img_size, img_size, 3), name='image')\n",
    "    x = base(inputs, training=False)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    encoded = layers.Dense(embed_dim, activation='relu', name='image_embedding')(x)\n",
    "    return Model(inputs, encoded, name='encoder')\n",
    "\n",
    "encoder = build_encoder()\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f8d046",
   "metadata": {},
   "source": [
    "## 6. GRU-based sequence decoder\n",
    "\n",
    "The decoder takes:\n",
    "\n",
    "- A sequence of token IDs (decoder input)\n",
    "- The image embedding (from the encoder)\n",
    "\n",
    "and produces logits over the vocabulary at each time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197cb402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_decoder(vocab_size, max_len, hidden_dim=256, embed_dim=128):\n",
    "    dec_input_tokens = layers.Input(shape=(max_len,), name='decoder_input')\n",
    "    image_feat       = layers.Input(shape=(256,),   name='image_embedding')\n",
    "\n",
    "    # Token embedding\n",
    "    x = layers.Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=embed_dim,\n",
    "        mask_zero=True,\n",
    "        name='token_embedding'\n",
    "    )(dec_input_tokens)\n",
    "\n",
    "    # Project image embedding to initial GRU state\n",
    "    init_state = layers.Dense(hidden_dim, activation='tanh')(image_feat)\n",
    "\n",
    "    gru_out = layers.GRU(\n",
    "        hidden_dim,\n",
    "        return_sequences=True,\n",
    "        name='decoder_gru'\n",
    "    )(x, initial_state=init_state)\n",
    "\n",
    "    logits = layers.Dense(vocab_size, name='vocab_logits')(gru_out)\n",
    "    return Model([dec_input_tokens, image_feat], logits, name='decoder')\n",
    "\n",
    "decoder = build_decoder(vocab_size, max_len, hidden_dim=256, embed_dim=128)\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6bd6ad",
   "metadata": {},
   "source": [
    "## 7. Full encoder–decoder model\n",
    "\n",
    "Combine the encoder and decoder into a single model that takes an image and the decoder input sequence, and outputs logits over the vocabulary at each time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4e5d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3), name='image')\n",
    "token_inputs = layers.Input(shape=(max_len,), name='decoder_input')\n",
    "\n",
    "img_emb = encoder(image_inputs)\n",
    "logits  = decoder([token_inputs, img_emb])\n",
    "\n",
    "model = Model(\n",
    "    inputs={'image': image_inputs, 'decoder_input': token_inputs},\n",
    "    outputs=logits,\n",
    "    name='image_to_fen_model'\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98a1192",
   "metadata": {},
   "source": [
    "## 8. Training\n",
    "\n",
    "We train the model with teacher forcing using sparse categorical cross-entropy on the sequence. The metric `token_accuracy` measures per-time-step accuracy (not full-sequence exact match)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d13a724",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "    loss=loss_fn,\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy(name='token_accuracy')],\n",
    ")\n",
    "\n",
    "# Adjust epochs as appropriate for your dataset size and resources.\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617a9b1f",
   "metadata": {},
   "source": [
    "## 9. Greedy decoding (inference)\n",
    "\n",
    "We now implement a simple greedy decoding loop to turn an image into a FEN-like string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2cd1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_tokens(token_ids):\n",
    "    chars_out = []\n",
    "    for tid in token_ids:\n",
    "        tid = int(tid)\n",
    "        if tid == char2idx[EOS_TOKEN] or tid == char2idx[PAD_TOKEN]:\n",
    "            break\n",
    "        if tid in idx2char and idx2char[tid] not in (SOS_TOKEN, EOS_TOKEN, PAD_TOKEN):\n",
    "            chars_out.append(idx2char[tid])\n",
    "    return ''.join(chars_out)\n",
    "\n",
    "def predict_fen_for_image(path):\n",
    "    # Prepare image\n",
    "    img = load_image(path)\n",
    "    img = tf.expand_dims(img, 0)  # add batch dim\n",
    "\n",
    "    # Encode image\n",
    "    img_emb = encoder(img, training=False)\n",
    "\n",
    "    # Start sequence with SOS\n",
    "    dec_input = np.zeros((1, max_len), dtype=np.int32)\n",
    "    dec_input[0, 0] = char2idx[SOS_TOKEN]\n",
    "\n",
    "    for t in range(1, max_len):\n",
    "        logits = decoder([dec_input, img_emb], training=False)\n",
    "        step_logits = logits[:, t-1, :]  # (1, vocab_size)\n",
    "        next_token = tf.argmax(step_logits, axis=-1).numpy()[0]\n",
    "        dec_input[0, t] = next_token\n",
    "        if next_token == char2idx[EOS_TOKEN]:\n",
    "            break\n",
    "\n",
    "    return decode_tokens(dec_input[0, 1:])  # skip SOS\n",
    "\n",
    "# Quick smoke test on a single image (if available)\n",
    "if test_paths:\n",
    "    sample_path = test_paths[0]\n",
    "    pred = predict_fen_for_image(sample_path)\n",
    "    print('Sample prediction :', pred)\n",
    "    print('Ground truth       :', Path(sample_path).stem)\n",
    "else:\n",
    "    print('No test images found; skipping sample prediction.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8a8de8",
   "metadata": {},
   "source": [
    "## 10. Exact-sequence evaluation\n",
    "\n",
    "We can now evaluate the model by computing exact string match accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33a0b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_exact_match(paths, labels, max_samples=None):\n",
    "    if max_samples is not None:\n",
    "        paths = paths[:max_samples]\n",
    "        labels = labels[:max_samples]\n",
    "\n",
    "    correct = 0\n",
    "    total = len(paths)\n",
    "\n",
    "    for p, true_label in zip(paths, labels):\n",
    "        pred = predict_fen_for_image(p)\n",
    "        if pred == true_label:\n",
    "            correct += 1\n",
    "\n",
    "    acc = correct / total if total > 0 else 0.0\n",
    "    print(f'Exact string match accuracy: {acc * 100:.2f}% ({correct}/{total})')\n",
    "    return acc\n",
    "\n",
    "# Example usage (limit max_samples for speed if needed):\n",
    "if test_paths:\n",
    "    evaluate_exact_match(test_paths, test_labels, max_samples=200)\n",
    "else:\n",
    "    print('No test data to evaluate.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
